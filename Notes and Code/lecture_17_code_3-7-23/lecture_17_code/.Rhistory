table(testNotes$Multilingualism)
table(testNotes$BehaviorCheck)
#Table based on Condition for Mean and SD of Final Score, Prior Knowledge, Enjoyment, Interest, Effort, and Difficulty
testNotes %>% group_by(Condition) %>% filter(BehaviorCheck == "No") %>%
mutate(avgScore = mean(FinalScore)) %>%
mutate(sdScore = sd(FinalScore)) %>%
mutate(avgPrior = mean(PriorKnowledge)) %>%
mutate(sdPrior = sd(PriorKnowledge)) %>%
mutate(avgEnjoyment = mean(Enjoyment)) %>%
mutate(sdEnjoyment = sd(Enjoyment)) %>%
mutate(avgInterest = mean(Interest)) %>%
mutate(sdInterest = sd(Interest)) %>%
mutate(avgEffort = mean(Effort)) %>%
mutate(sdEffort = sd(Effort)) %>%
mutate(avgDifficulty = mean(Difficulty)) %>%
mutate(sdDifficulty = sd(Difficulty)) %>%
select(avgScore,sdScore,avgPrior,sdPrior,avgEnjoyment,sdEnjoyment,avgInterest,sdInterest,avgEffort,sdEffort,avgDifficulty,sdDifficulty) %>%
arrange(factor(Condition, levels = c("NoNoteTaking","RegularNoteTaking","InterpolatedNoteTaking"
))) %>%
distinct()
#One Way Anovas
testNotesBehaved <- testNotes %>% filter(BehaviorCheck == "No")
priorKnowledgeAOV <- aov(PriorKnowledge ~ Condition, data = testNotesBehaved)
summary(priorKnowledgeAOV)
eta_squared(priorKnowledgeAOV)
enjoymentAOV <- aov(Enjoyment ~ Condition, data = testNotesBehaved)
summary(enjoymentAOV)
eta_squared(enjoymentAOV)
interestAOV <- aov(Interest ~ Condition, data = testNotesBehaved)
summary(interestAOV)
eta_squared(interestAOV)
effortAOV <- aov(Effort ~ Condition, data = testNotesBehaved)
summary(effortAOV)
eta_squared(effortAOV)
difficultyAOV <- aov(Difficulty ~ Condition, data = testNotesBehaved)
summary(difficultyAOV)
eta_squared(difficultyAOV)
##Descriptives of Pre and Post JOL and Scores)
testNotes %>% group_by(Condition) %>% filter(BehaviorCheck == "No") %>%
mutate(avgPreJOL = mean(`pre-JOL`)) %>%
mutate(sdPreJOL = sd(`pre-JOL`)) %>%
mutate(avgPreJOLDelay = mean(`pre-JOL_retention`)) %>%
mutate(sdPreJOLDelay = sd(`pre-JOL_retention`)) %>%
mutate(avgPostConf = mean(`post-confidence`)) %>%
mutate(sdPostConf = sd(`post-confidence`)) %>%
mutate(avgScore = mean(FinalScore)) %>%
mutate(sdScore = sd(FinalScore)) %>%
select(avgPreJOL,sdPreJOL,avgPreJOLDelay,sdPreJOLDelay,avgPostConf,sdPostConf,avgScore,sdScore) %>%
arrange(factor(Condition, levels = c("NoNoteTaking","RegularNoteTaking","InterpolatedNoteTaking"
))) %>%
distinct()
#One Way Anovas
preJOLAOV <-  aov(testNotesBehaved$'pre-JOL' ~ Condition, data = testNotesBehaved)
summary(preJOLAOV)
eta_squared(preJOLAOV)
preJOLDelayAOV <- aov(testNotesBehaved$'pre-JOL_retention' ~ Condition, data = testNotesBehaved)
summary(preJOLDelayAOV)
eta_squared(preJOLDelayAOV)
postConfAOV <- aov(testNotesBehaved$'post-confidence' ~ Condition, data = testNotesBehaved)
summary(postConfAOV)
eta_squared(postConfAOV)
finalScoreAOV <- aov(FinalScore ~ Condition, data = testNotesBehaved)
summary(finalScoreAOV)
eta_squared(finalScoreAOV)
#Read in the 2 datasets
setwd("~/UT Austin/Research/Interpolated Note-Taking/ALD Surveys")
finalGrades <- read_csv("FinalGradesCSV.csv")
beginningSurvey <- read_csv("BeginningSurvey.csv")
finalGrades <- finalGrades %>% mutate(Percentage = as.numeric(gsub("%$","",finalGrades$Percentage))) %>% rename(id = ID)
#Merge the datasets together by id and rename the columns
aldSurvey <- left_join(beginningSurvey,finalGrades, by = "id")
aldSurveySubset <- aldSurvey %>% select(id,`28202139: In general, how will you most likely take notes for this class?`,
`28202140: Which of the following best describes the way you tend to take notes?`,
`28202143: How do you use your notes? Check all that apply.`,
`28202144: If you feel like the last couple of questions did not fully capture (a) how you tend to take notes, and (b) how you use your notes, please provide more detail in the space below.`,Percentage) %>%
rename(noteTakingStrat = `28202139: In general, how will you most likely take notes for this class?`,
bestDescription = `28202140: Which of the following best describes the way you tend to take notes?`,
useNotes = `28202143: How do you use your notes? Check all that apply.`,
extraDetail = `28202144: If you feel like the last couple of questions did not fully capture (a) how you tend to take notes, and (b) how you use your notes, please provide more detail in the space below.`)
#Convert awkward N/A's to true NA
aldSurveySubset[aldSurveySubset == "n/a"] <- NA
aldSurveySubset[aldSurveySubset == "N/A"] <- NA
aldSurveySubset[aldSurveySubset == "N/a"] <- NA
#Recode the Categorical Entries?
table(aldSurveySubset$noteTakingStrat)
table(aldSurveySubset$bestDescription)
table(aldSurveySubset$useNotes) #Qualitiative
table(aldSurveySubset$extraDetail) #Qualitative
a <- aldSurveySubset %>% group_by(noteTakingStrat) %>%
summarise(cnt = n()) %>%
mutate(freq = cnt / sum(cnt)) %>%
rename(Strategy = noteTakingStrat)
b <- aldSurveySubset %>% group_by(bestDescription) %>%
summarise(cnt = n()) %>%
mutate(freq = cnt / sum(cnt)) %>%
rename(Strategy = bestDescription)
aldDescriptives <-rbind(a,b)
aldDescriptives <-rbind(a,b)
aldDescriptives
aldDescriptives
a
knitr::opts_chunk$set(echo = TRUE)
aldDescriptives
aldDescriptives
install.packages(LKT)
install.packages("LKT")
library(LKT)
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code/Bernoulli")
###
###  Read in Coexistence Data
###
#
#  Hein, C.L., Ã–hlund, G. and Englund, G., 2014. Fish introductions reveal the temperature dependence of species interactions. Proceedings of the Royal Society B: Biological Sciences, 281(1775), p.20132641.
#
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code/Bernoulli")
logit<-function(theta){log(theta)-log(1-theta)}
logit.inv<-function(x){exp(x)/(1+exp(x))}
ce.df=read.table("coexist.txt",header=TRUE)
head(ce.df)
n=dim(ce.df)[1]
y=ce.df$coexist
p=4
X=matrix(1,n,p)
X=matrix(1,n,p)
X[,2]=scale(ce.df$elev)
X[,3]=scale(ce.df$maxdepth)
X[,4]=scale(ce.df$temp1)
pairs(cbind(y,X[,-1]))
summary(glm(y~0+X,family="binomial"))  # check MLE
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(theta[i],N)
logit(theta[i]) <- b0+b1*X[i,2]+b2*X[i,3]+b3*X[i,4]
}
b0 ~ dnorm(mu,tau)
b1 ~ dnorm(mu,tau)
b2 ~ dnorm(mu,tau)
b3 ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/2.25
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'N'=1,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
n.mcmc=50000
n.burn=round(.2*n.mcmc)
update(m.out,n.burn) # perform burn-in
m.samples=jags.samples(m.out,c('b0','b1','b2','b3'),n.mcmc) # fit model post-burn-in
beta <- rnorm(10000, 0, sqrt(2.25))
hist(beta)
hist(bquote[beta])
hist(beta)
#Implied Prior?
logit<-function(theta){log(theta)-log(1-theta)}
logit.inv<-function(x){exp(x)/(1+exp(x))}
hist(logit.inv(beta))
hist(logit.inv(rnorm(10000, 0, sqrt(100))))
hist(logit.inv(rnorm(10000, 0, sqrt(2.25))))
hist(logit.inv(rnorm(10000, 0, sqrt(100))))
hist(logit.inv(rnorm(100000, 0, sqrt(2.25))))
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code/Bernoulli")
logit<-function(theta){log(theta)-log(1-theta)}
logit.inv<-function(x){exp(x)/(1+exp(x))}
ce.df=read.table("coexist.txt",header=TRUE)
head(ce.df)
n=dim(ce.df)[1]
y=ce.df$coexist
p=4
X=matrix(1,n,p)
X[,2]=scale(ce.df$elev)
X[,3]=scale(ce.df$maxdepth)
X[,4]=scale(ce.df$temp1)
pairs(cbind(y,X[,-1]))
summary(glm(y~0+X,family="binomial"))  # check MLE
###
###  Specify Binary Regression Model in JAGS
###
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(theta[i],N)
logit(theta[i]) <- b0+b1*X[i,2]+b2*X[i,3]+b3*X[i,4]
}
b0 ~ dnorm(mu,tau)
b1 ~ dnorm(mu,tau)
b2 ~ dnorm(mu,tau)
b3 ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/2.25 #Precision. 1 / sigma^2
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'N'=1,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
n.mcmc=50000
n.burn=round(.2*n.mcmc)
update(m.out,n.burn) # perform burn-in
m.samples=jags.samples(m.out,c('b0','b1','b2','b3'),n.mcmc) # fit model post-burn-in
beta.post.mat=cbind(m.samples$b0[1,,1],m.samples$b1[1,,1],m.samples$b2[1,,1],m.samples$b3[1,,1])
matplot(beta.post.mat,type="l",lty=1,ylab=bquote(beta))
library(vioplot)
vioplot(data.frame(beta.post.mat),names=expression(beta[0],beta[1],beta[2],beta[3]))
abline(h=0,col=8)
apply(beta.post.mat,2,mean) # marginal posterior means for beta
apply(beta.post.mat,2,sd) # marginal posterior means for beta
apply(beta.post.mat,2,quantile,c(0.025,.975)) # marginal posterior 95% CI for beta
vioplot(data.frame(beta.post.mat),names=expression(beta[0],beta[1],beta[2],beta[3]))
library(vioplot)
install.packages(vioplot)
apply(beta.post.mat,2,mean) # marginal posterior means for beta
apply(beta.post.mat,2,sd) # marginal posterior means for beta
apply(beta.post.mat,2,quantile,c(0.025,.975)) # marginal posterior 95% CI for beta
tmax.df=read.csv("laboratory_CTM.csv")
head(tmax.df)
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code/Normal")
###
###  Read in T_max Data
###
#
#  Troia, M.J. and Giam, X., 2019. Extreme heat events and the vulnerability of endemic montane fishes to climate change. Ecography, 42(11), pp.1913-1925.
#
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code/Normal")
tmax.df=read.csv("laboratory_CTM.csv")
head(tmax.df)
###
###  Read in T_max Data
###
#
#  Troia, M.J. and Giam, X., 2019. Extreme heat events and the vulnerability of endemic montane fishes to climate change. Ecography, 42(11), pp.1913-1925.
#
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code/Normal")
tmax.df=read.csv("laboratory_CTM.csv")
head(tmax.df)
n=dim(tmax.df)[1]
y=c(tmax.df$LRR_sustained)
p=5
X=matrix(1,n,p)
X[,1:4]=model.matrix(~0+tmax.df$species)
X[,5]=tmax.df$acclim_temp
pairs(cbind(y,X[,-1]))
summary(lm(y~0+X))  # check least squares fit
X
~0+tmax.df$species
model.matrix(~0+tmax.df$species)
X[,1:4]=model.matrix(~0+tmax.df$species) #Dummy variable for each species. 1 or a 0. 4 different speciies
X[,5]=tmax.df$acclim_temp
pairs(cbind(y,X[,-1]))
summary(lm(y~0+X))  # check least squares fit
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dnorm(mu[i], tau)  # note: tau is precision=1/variance!
mu[i] = X[i,1:p]%*%beta
}
beta ~ dmnorm(mu.beta,Tau.beta)
tau <- pow(sig, -2)
sig ~ dunif(0,10)
}
"
mod<-textConnection(m.jags)
mu.beta=rep(0,p)
Sig.beta=10000*diag(p)
Tau.beta=solve(Sig.beta)
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'p'=p,'mu.beta'=mu.beta,'Tau.beta'=Tau.beta),n.chains=1,n.adapt=0) # build model and algorithm
n.mcmc=50000
n.burn=round(.2*n.mcmc)
update(m.out,n.burn) # perform burn-in
m.samples=jags.samples(m.out,c('beta','sig'),n.mcmc) # fit model post-burn-in
layout(matrix(1:2,2,1))
matplot(t(m.samples$beta[,,1]),type="l",lty=1,ylab=bquote(beta))
plot(m.samples$sig[1,,1],type="l",lty=1,ylab=bquote(sigma))
apply(m.samples$beta[,,1],1,mean) # marginal posterior means for beta
apply(m.samples$beta[,,1],1,sd) # marginal posterior sd for beta
apply(m.samples$beta[,,1],1,quantile,c(0.025,.975)) # marginal posterior 95% CI for beta
mean(m.samples$beta[1,,1]>m.samples$beta[2,,1]) # Post prob: P(beta_1 > beta_2 | y) = 0.99
plot(m.samples$beta[1,,1],m.samples$beta[2,,1],col=rgb(0,0,0,.1),pch=16,cex=.5,asp=TRUE) # don't forget about joint inference!
abline(0,1,col=2)
n=30
p=3
X=matrix(1,n,p)
set.seed(101)
X[,2]=rnorm(n)
X[,3]=rnorm(n)
beta=c(.5,-1,2)
lambda=exp(X%*%beta)
y=rpois(n,lambda)  # simulate data
pairs(cbind(y,X[,-1]))
summary(glm(y~0+X,family=poisson(link="log")))  # check MLE
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
lambda[i] = exp(X[i,1:p]%*%beta)
}
beta ~ dmnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
n=30
p=3
X=matrix(1,n,p)
set.seed(101)
X[,2]=rnorm(n)
X[,3]=rnorm(n)
beta=c(.5,-1,2)
lambda=exp(X%*%beta)
y=rpois(n,lambda)  # simulate data
pairs(cbind(y,X[,-1]))
summary(glm(y~0+X,family=poisson(link="log")))  # check MLE
###
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
lambda[i] = exp(X[i,1:p]%*%beta)
}
beta ~ dmnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code")
###
###  Simulate Count Regression Data
###
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code")
n=30
p=3
X=matrix(1,n,p)
set.seed(101)
X[,2]=rnorm(n)
X[,3]=rnorm(n)
beta=c(.5,-1,2)
lambda=exp(X%*%beta)
y=rpois(n,lambda)  # simulate data
pairs(cbind(y,X[,-1]))
summary(glm(y~0+X,family=poisson(link="log")))  # check MLE
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
lambda[i] = exp(X[i,1:p]%*%beta)
}
beta ~ dmnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
lambda[i] = exp(X[i,1:p]%*%beta)
}
beta ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
log(lambda[i]) = X[i,1:p]%*%beta
}
beta ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau, 'p'=p),n.chains=1,n.adapt=0) # build model and algorithm
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau, 'p'=p),n.chains=1,n.adapt=0) # build model and algorithm
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
lambda[i] = exp(X[i,1:3]%*%beta)
}
beta ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
log(lambda[i]) = X[i,1:3]%*%beta
}
beta ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
log(lambda[i]) = X[i,1:p]%*%beta
}
beta ~ dnorm(mu,tau)
}
"
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau, 'p'=p),n.chains=1,n.adapt=0) # build model and algorithm
###
###  Simulate Count Regression Data
###
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_17_code/lecture_17_code")
n=30
p=3
X=matrix(1,n,p)
set.seed(101)
X[,2]=rnorm(n)
X[,3]=rnorm(n)
beta=c(.5,-1,2)
lambda=exp(X%*%beta)
y=rpois(n,lambda)  # simulate data
pairs(cbind(y,X[,-1]))
summary(glm(y~0+X,family=poisson(link="log")))  # check MLE
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
log(lambda[i]) = X[i,1:p]%*%beta
}
beta ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau,'p'=p),n.chains=1,n.adapt=0) # build model and algorithm
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
log(lambda[i]) = b0 + b1*X[i,2] + b2*X[i,3]
}
b0 ~ dnorm(mu,tau)
b1 ~ dnorm(mu,tau)
b2 ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
n.mcmc=10000
n.burn=round(.2*n.mcmc)
update(m.out,n.burn) # perform burn-in
m.samples=jags.samples(m.out,c('b0','b1','b2'),n.mcmc) # fit model post-burn-in
beta.post.mat=cbind(m.samples$b0[1,,1],m.samples$b1[1,,1],m.samples$b2[1,,1])
matplot(beta.post.mat,type="l",lty=1,ylab=bquote(beta))
abline(h=beta,col=8)
m.jags_matrix <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
log(lambda[i]) = X[i,1:p]*beta
}
beta ~ dnorm(mu,tau)
}
"
mod_matrix <- textConnection(m.jags_matrix)
m.out1 <-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau,'p'=p),n.chains=1,n.adapt=0) # build model and algorithm
m.out1 <-jags.model(mod_matrix,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau,'p'=p),n.chains=1,n.adapt=0) # build model and algorithm
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dpois(lambda[i])
log(lambda[i]) = b0 + b1*X[i,2] + b2*X[i,3]
}
b0 ~ dnorm(mu,tau)
b1 ~ dnorm(mu,tau)
b2 ~ dnorm(mu,tau)
}
"
mod<-textConnection(m.jags)
mu=0
tau=1/100
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'X'=X,'mu'=mu,'tau'=tau),n.chains=1,n.adapt=0) # build model and algorithm
n.mcmc=10000
n.burn=round(.2*n.mcmc)
update(m.out,n.burn) # perform burn-in
m.samples=jags.samples(m.out,c('b0','b1','b2'),n.mcmc) # fit model post-burn-in
beta.post.mat=cbind(m.samples$b0[1,,1],m.samples$b1[1,,1],m.samples$b2[1,,1])
matplot(beta.post.mat,type="l",lty=1,ylab=bquote(beta))
abline(h=beta,col=8)
