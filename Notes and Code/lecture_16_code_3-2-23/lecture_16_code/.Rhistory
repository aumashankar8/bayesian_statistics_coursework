mutate(effectiveness_3 = as.numeric(effectiveness_3))
#Descriptives of Mean and SD for each Group
aggregate(lapply(testNotes[9:13],as.numeric), list(testNotes$Condition),mean, na.rm=TRUE)
aggregate(lapply(testNotes[9:13],as.numeric), list(testNotes$Condition),sd, na.rm=TRUE)
# Descriptives:
length(which(testNotes$Condition == "NoNoteTaking")) #86
length(which(testNotes$Condition == "RegularNoteTaking")) #80
length(which(testNotes$Condition == "InterpolatedNoteTaking")) #86
# Demographics
aggregate(lapply(testNotes[32:34],as.numeric), list(testNotes$Condition),mean, na.rm=TRUE)
table(testNotes$age)
table(testNotes$sex)
table(testNotes$GenPref)
table(testNotes$ethnicity)
table(testNotes$`Year in college`)
table(testNotes$majorClassification)
table(testNotes$`English Fluency`)
table(testNotes$Multilingualism)
table(testNotes$BehaviorCheck)
#Table based on Condition for Mean and SD of Final Score, Prior Knowledge, Enjoyment, Interest, Effort, and Difficulty
testNotes %>% group_by(Condition) %>% filter(BehaviorCheck == "No") %>%
mutate(avgScore = mean(FinalScore)) %>%
mutate(sdScore = sd(FinalScore)) %>%
mutate(avgPrior = mean(PriorKnowledge)) %>%
mutate(sdPrior = sd(PriorKnowledge)) %>%
mutate(avgEnjoyment = mean(Enjoyment)) %>%
mutate(sdEnjoyment = sd(Enjoyment)) %>%
mutate(avgInterest = mean(Interest)) %>%
mutate(sdInterest = sd(Interest)) %>%
mutate(avgEffort = mean(Effort)) %>%
mutate(sdEffort = sd(Effort)) %>%
mutate(avgDifficulty = mean(Difficulty)) %>%
mutate(sdDifficulty = sd(Difficulty)) %>%
select(avgScore,sdScore,avgPrior,sdPrior,avgEnjoyment,sdEnjoyment,avgInterest,sdInterest,avgEffort,sdEffort,avgDifficulty,sdDifficulty) %>%
arrange(factor(Condition, levels = c("NoNoteTaking","RegularNoteTaking","InterpolatedNoteTaking"
))) %>%
distinct()
#One Way Anovas
testNotesBehaved <- testNotes %>% filter(BehaviorCheck == "No")
priorKnowledgeAOV <- aov(PriorKnowledge ~ Condition, data = testNotesBehaved)
summary(priorKnowledgeAOV)
eta_squared(priorKnowledgeAOV)
enjoymentAOV <- aov(Enjoyment ~ Condition, data = testNotesBehaved)
summary(enjoymentAOV)
eta_squared(enjoymentAOV)
interestAOV <- aov(Interest ~ Condition, data = testNotesBehaved)
summary(interestAOV)
eta_squared(interestAOV)
effortAOV <- aov(Effort ~ Condition, data = testNotesBehaved)
summary(effortAOV)
eta_squared(effortAOV)
difficultyAOV <- aov(Difficulty ~ Condition, data = testNotesBehaved)
summary(difficultyAOV)
eta_squared(difficultyAOV)
##Descriptives of Pre and Post JOL and Scores)
testNotes %>% group_by(Condition) %>% filter(BehaviorCheck == "No") %>%
mutate(avgPreJOL = mean(`pre-JOL`)) %>%
mutate(sdPreJOL = sd(`pre-JOL`)) %>%
mutate(avgPreJOLDelay = mean(`pre-JOL_retention`)) %>%
mutate(sdPreJOLDelay = sd(`pre-JOL_retention`)) %>%
mutate(avgPostConf = mean(`post-confidence`)) %>%
mutate(sdPostConf = sd(`post-confidence`)) %>%
mutate(avgScore = mean(FinalScore)) %>%
mutate(sdScore = sd(FinalScore)) %>%
select(avgPreJOL,sdPreJOL,avgPreJOLDelay,sdPreJOLDelay,avgPostConf,sdPostConf,avgScore,sdScore) %>%
arrange(factor(Condition, levels = c("NoNoteTaking","RegularNoteTaking","InterpolatedNoteTaking"
))) %>%
distinct()
#One Way Anovas
preJOLAOV <-  aov(testNotesBehaved$'pre-JOL' ~ Condition, data = testNotesBehaved)
summary(preJOLAOV)
eta_squared(preJOLAOV)
preJOLDelayAOV <- aov(testNotesBehaved$'pre-JOL_retention' ~ Condition, data = testNotesBehaved)
summary(preJOLDelayAOV)
eta_squared(preJOLDelayAOV)
postConfAOV <- aov(testNotesBehaved$'post-confidence' ~ Condition, data = testNotesBehaved)
summary(postConfAOV)
eta_squared(postConfAOV)
finalScoreAOV <- aov(FinalScore ~ Condition, data = testNotesBehaved)
summary(finalScoreAOV)
eta_squared(finalScoreAOV)
#Read in the 2 datasets
setwd("~/UT Austin/Research/Interpolated Note-Taking/ALD Surveys")
finalGrades <- read_csv("FinalGradesCSV.csv")
beginningSurvey <- read_csv("BeginningSurvey.csv")
finalGrades <- finalGrades %>% mutate(Percentage = as.numeric(gsub("%$","",finalGrades$Percentage))) %>% rename(id = ID)
#Merge the datasets together by id and rename the columns
aldSurvey <- left_join(beginningSurvey,finalGrades, by = "id")
aldSurveySubset <- aldSurvey %>% select(id,`28202139: In general, how will you most likely take notes for this class?`,
`28202140: Which of the following best describes the way you tend to take notes?`,
`28202143: How do you use your notes? Check all that apply.`,
`28202144: If you feel like the last couple of questions did not fully capture (a) how you tend to take notes, and (b) how you use your notes, please provide more detail in the space below.`,Percentage) %>%
rename(noteTakingStrat = `28202139: In general, how will you most likely take notes for this class?`,
bestDescription = `28202140: Which of the following best describes the way you tend to take notes?`,
useNotes = `28202143: How do you use your notes? Check all that apply.`,
extraDetail = `28202144: If you feel like the last couple of questions did not fully capture (a) how you tend to take notes, and (b) how you use your notes, please provide more detail in the space below.`)
#Convert awkward N/A's to true NA
aldSurveySubset[aldSurveySubset == "n/a"] <- NA
aldSurveySubset[aldSurveySubset == "N/A"] <- NA
aldSurveySubset[aldSurveySubset == "N/a"] <- NA
#Recode the Categorical Entries?
table(aldSurveySubset$noteTakingStrat)
table(aldSurveySubset$bestDescription)
table(aldSurveySubset$useNotes) #Qualitiative
table(aldSurveySubset$extraDetail) #Qualitative
a <- aldSurveySubset %>% group_by(noteTakingStrat) %>%
summarise(cnt = n()) %>%
mutate(freq = cnt / sum(cnt)) %>%
rename(Strategy = noteTakingStrat)
b <- aldSurveySubset %>% group_by(bestDescription) %>%
summarise(cnt = n()) %>%
mutate(freq = cnt / sum(cnt)) %>%
rename(Strategy = bestDescription)
aldDescriptives <-rbind(a,b)
aldDescriptives <-rbind(a,b)
aldDescriptives
aldDescriptives
a
knitr::opts_chunk$set(echo = TRUE)
aldDescriptives
aldDescriptives
install.packages(LKT)
install.packages("LKT")
library(LKT)
mf.df=read.table("mosquitofish.txt",header=TRUE) # read in mosquitofish data
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_16_code_3-2-23/lecture_16_code")
###
###  Read in Data for Binomial Model
###
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_16_code_3-2-23/lecture_16_code")
mf.df=read.table("mosquitofish.txt",header=TRUE) # read in mosquitofish data
head(mf.df)
n=32 # those groups of fish that were only counted twice
y=mf.df$N1[1:n]
N=mf.df$N0[1:n]
plot(N,y,asp=TRUE)
abline(0,1)
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=1000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1),n.chains=1,n.adapt=0) # build model graph
m.samples=jags.samples(m.out,c('p'),n.mcmc) # fit model to data
plot(m.samples$p[1,,1],type="l",lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plot
hist(m.samples$p[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
lines(density(m.samples$p[1,n.burn:n.mcmc,1]),lwd=2,col=2)
library(rjags)
###
###  Read in Data for Binomial Model
###
setwd("~/UT Austin/Spring 2023/Bayesian Stats/Notes and Code/lecture_16_code_3-2-23/lecture_16_code")
mf.df=read.table("mosquitofish.txt",header=TRUE) # read in mosquitofish data
head(mf.df)
n=32 # those groups of fish that were only counted twice
y=mf.df$N1[1:n]
N=mf.df$N0[1:n]
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=1000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model, do the model statement in another script
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1),n.chains=1,n.adapt=0) # build model graph | n.cahins is amount of models it will produce and then converge | n.adapt is number of adaptations to adapt. n
m.samples=jags.samples(m.out,c('p'),n.mcmc) # fit model to data
#jags.samples(model, which var to write out, how many mcmc iterations)
#Output. p parameter[which p,which mcmc iters (all of em), which chain]
plot(m.samples$p[1,,1],type="l",lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) #
hist(m.samples$p[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
mean(m.samples$p[1,n.burn:n.mcmc,1]) #Posterior mean = 0.871
lines(density(m.samples$p[1,n.burn:n.mcmc,1]),lwd=2,col=2)
library(rjags)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
}
"
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1),n.chains=3,n.adapt=0) # build model graph
m.samples=jags.samples(m.out,c('p'),n.mcmc) # fit model to data
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
keep.idx=n.burn:n.mcmc # discard burn-in
K=length(keep.idx)
w=mean(apply(m.samples$p[1,keep.idx,],2,var))
b=K*var(apply(m.samples$p[1,keep.idx,],2,mean))
v.hat=w*(K-1)/K+b/K
r.hat=sqrt(v.hat/w)
r.hat
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1),n.chains=3,n.adapt=0) # build model graph
mod
m.out
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
}
"
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1),n.chains=3,n.adapt=0) # build model graph
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1),n.chains=3,n.adapt=0) # build model graph
m.samples=jags.samples(m.out,c('p'),n.mcmc) # fit model to data
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
library(rjags)
library(coda)
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=100
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1),n.chains=3,n.adapt=0) # build model graph
m.samples=coda.samples(m.out,c('p'),n.mcmc) # fit model to data
gelman.diag(m.samples)
m.samples
gelman.diag(m.samples)
str(m.samples)
m.samples$p
m.samples
m.samples[[1]]
m.samples[[1]]$p
m.samples$p[[1]]
m.samples[[1]]
m.samples[[1]]$p
plot(m.samples[[1]], type = "l")
m.samples[[1,1]]
as.numeric(m.samples[[1]]$p)
m.samples
m.samples[[1]]
?coda.samples
summary(m.samples)
coda.options()
?coda.options
?coda
?codamenu
codamenu()
n=10
lambda=20
N=rpois(n,lambda)
p=.8
y=rbinom(n,N,p)
plot(N,y,xlim=c(0,max(N)),ylim=c(0,max(N)))
abline(0,1)
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
for(i in 1:n){
N[i] ~ rpois(lambda)
}
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1, 'lambda'=1),n.chains=3,n.adapt=0) # build model graph
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
for(i in 1:n){
N[i] ~ dpois(lambda)
}
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'N'=N,'alpha'=1,'beta'=1, 'lambda'=1),n.chains=3,n.adapt=0) # build model graph
m.samples <- jags.samples(m.out, c('p','N'), n.mcmc)
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
m.samples
m.samples <- jags.samples(m.out, c('p','N[i]'), n.mcmc)
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'alpha'=1,'beta'=1,'lambda'=1),n.chains=3,n.adapt=0) # build model graph
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
}
p ~ dbeta(alpha,beta)
for(i in 1:n){
N[i] ~ dpois(lambda)
}
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'alpha'=1,'beta'=1,'lambda'=1),n.chains=3,n.adapt=0) # build model graph
m.samples <- jags.samples(m.out, c('p','N'), n.mcmc)
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
m.samples$N
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
hist(m.samples$p[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
hist(m.samples$N[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
m.samples$N
lines(density(m.samples$p[1,n.burn:n.mcmc,1]),lwd=2,col=2)
hist(m.samples$N[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
hist(m.samples$p[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration",ylim=c(0,1)) # trace plots
mean(m.samples$p[1,n.burn:n.mcmc,1]) #Posterior mean = 0.874
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
N[i] ~ dpois(lambda)
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'alpha'=1,'beta'=1,'lambda'=1),n.chains=3,n.adapt=0) # build model graph
m.samples <- jags.samples(m.out, c('p','N'), n.mcmc)
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
mean(m.samples$p[1,n.burn:n.mcmc,1]) #Posterior mean = 0.874
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
N[i] ~ dpois(lambda)
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'alpha'=1,'beta'=1,'lambda'=20),n.chains=3,n.adapt=0) # build model graph
m.samples <- jags.samples(m.out, c('p','N'), n.mcmc)
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration",ylim=c(0,1)) # trace plots
mean(m.samples$p[1,n.burn:n.mcmc,1]) #Posterior mean = 0.874
hist(m.samples$p[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
hist(m.samples$N[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
hist(m.samples$p[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram of p
hist(m.samples$N[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram of N
m.samples$N
n=10
lambda=20
N=rpois(n,lambda)
p=.8
y=rbinom(n,N,p)
plot(N,y,xlim=c(0,max(N)),ylim=c(0,max(N)))
abline(0,1)
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
N[i] ~ dpois(lambda)
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'alpha'=1,'beta'=1,'lambda'=lambda),n.chains=3,n.adapt=0) # build model graph
m.samples <- jags.samples(m.out, c('p','N'), n.mcmc)
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plots
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration",ylim=c(0,1)) # trace plots
mean(m.samples$p[1,n.burn:n.mcmc,1]) #Posterior mean = 0.874
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
N[i] ~ dpois(lambda)
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'alpha'=1,'beta'=1,'lambda'=lambda),n.chains=3,n.adapt=0) # build model graph
m.samples <- jags.samples(m.out, c('p','N'), n.mcmc)
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plot for p
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration",ylim=c(0,1)) # trace plot for N
matplot(m.samples$N[1,,1],type="l",col=1:3,lty=1,ylab="N",xlab="iteration",ylim=c(0,1)) # trace plot for N
matplot(m.samples$N[1,,1],type="l",col=1:3,lty=1,ylab="N",xlab="iteration") # trace plot for N
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration") # trace plot for N
hist(m.samples$N[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram of N
hist(m.samples$N[1,n.burn:n.mcmc,1],breaks=30,prob=TRUE,xlab="N",ylab="density",main="") # posterior histogram of N
hist(m.samples$p[1,n.burn:n.mcmc,1],breaks=30,xlim=c(0,1),prob=TRUE,xlab="p",ylab="density",main="") # posterior histogram of p
hist(m.samples$N[1,n.burn:n.mcmc,1],breaks=30,prob=TRUE,xlab="N",ylab="density",main="") # posterior histogram of N
matplot(m.samples$N[2,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration") # trace plot for N# trace plot for N
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration") # trace plot for N# trace plot for N
matplot(m.samples$N[2,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration") # trace plot for N# trace plot for N
matplot(m.samples$N[4,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration") # trace plot for N# trace plot for N
matplot(m.samples$N[1,,],type="l",col=1:3,lty=1,ylab="N",xlab="iteration") # trace plot for N# trace plot for N
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plot for p
matplot(m.samples$p[2,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plot for p
matplot(m.samples$p[4,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plot for p
matplot(m.samples$p[1,,],type="l",col=1:3,lty=1,ylab="p",xlab="iteration",ylim=c(0,1)) # trace plot for p
m.samples$N
#For R_Hat
keep.idx=n.burn:n.mcmc # discard burn-in
K=length(keep.idx)
w=mean(apply(m.samples$p[1,keep.idx,],2,var))
b=K*var(apply(m.samples$p[1,keep.idx,],2,mean))
v.hat=w*(K-1)/K+b/K
r.hat=sqrt(v.hat/w)
r.hat
#R_hat for N
keep.idx=n.burn:n.mcmc # discard burn-in
K=length(keep.idx)
w=mean(apply(m.samples$N[1,keep.idx,],2,var))
b=K*var(apply(m.samples$N[1,keep.idx,],2,mean))
v.hat=w*(K-1)/K+b/K
r.hat=sqrt(v.hat/w)
r.hat
###
###  Fit Hierarchical Binomial-Poisson Model assuming p and all N Unknown (but lambda known)
###
m.jags <-"
model{
for(i in 1:n){
y[i] ~ dbin(p,N[i])
N[i] ~ dpois(lambda)
}
p ~ dbeta(alpha,beta)
}
"
n.mcmc=10000
n.burn=round(.2*n.mcmc)
mod<-textConnection(m.jags) # read model
m.out<-jags.model(mod,data=list('y'=y,'n'=n,'alpha'=1,'beta'=1,'lambda'=lambda),n.chains=3,n.adapt=0) # build model graph
#Coda Sample
m.samples.coda <- coda.samples(m.out, c('p','N'), n.mcmc)
gelman.diag(m.samples.coda)
summary(m.samples.coda)
gelman.diag(m.samples.coda)
